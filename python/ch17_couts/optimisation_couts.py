"""
Chapitre 17 ‚Äî Optimisation des co√ªts
======================================
1. Calculateur de co√ªt par requ√™te
2. Prompt caching Anthropic
3. Cascade avec score de confiance
4. Batch processing Anthropic (‚àí50%)
5. Router multi-provider avec circuit breaker

D√©pendances : anthropic, openai
"""

from __future__ import annotations

import os
import time
import json
import asyncio
from collections import deque
from dataclasses import dataclass
from enum import Enum

import anthropic
from openai import AsyncOpenAI

aclient_oai  = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))
client_anth  = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY", ""))


# ---------------------------------------------------------------------------
# 1. Calculateur de co√ªt
# ---------------------------------------------------------------------------

@dataclass
class ModelPricing:
    name:                  str
    input_per_million:     float   # USD
    output_per_million:    float   # USD
    cached_per_million:    float = 0.0  # prompt cache hit


PRICING = {
    "claude-3-5-haiku":    ModelPricing("claude-3-5-haiku",    0.80,  4.00, 0.08),
    "claude-3-5-sonnet":   ModelPricing("claude-3-5-sonnet",   3.00, 15.00, 0.30),
    "claude-opus-4":       ModelPricing("claude-opus-4",       15.00, 75.00, 1.50),
    "gpt-4o-mini":         ModelPricing("gpt-4o-mini",         0.15,  0.60),
    "gpt-4o":              ModelPricing("gpt-4o",              2.50, 10.00),
}


def estimate_cost(
    model:          str,
    input_tokens:   int,
    output_tokens:  int,
    cached_tokens:  int = 0,
) -> float:
    """Retourne le co√ªt estim√© en USD."""
    p              = PRICING[model]
    fresh_tokens   = input_tokens - cached_tokens
    cost_input     = fresh_tokens  / 1_000_000 * p.input_per_million
    cost_cached    = cached_tokens / 1_000_000 * p.cached_per_million
    cost_output    = output_tokens / 1_000_000 * p.output_per_million
    return cost_input + cost_cached + cost_output


def compare_models(input_tokens: int, output_tokens: int):
    print(f"\n=== Co√ªt pour {input_tokens:,} tokens in / {output_tokens:,} tokens out ===\n")
    print(f"  {'Mod√®le':<25} {'Co√ªt USD':>12}   {'Co√ªt 10k req':>14}")
    print("  " + "‚îÄ" * 55)
    for model in PRICING:
        c    = estimate_cost(model, input_tokens, output_tokens)
        c10k = c * 10_000
        print(f"  {model:<25} ${c:>11.4f}   ${c10k:>13.2f}")


# ---------------------------------------------------------------------------
# 2. Prompt caching Anthropic
# ---------------------------------------------------------------------------

SYSTEM_PROMPT_LONG = (
    "Tu es un assistant expert en droit du travail fran√ßais. "
    "Voici la base documentaire de r√©f√©rence : " + "Lorem ipsum " * 500  # simuler un long system prompt
)


def anthropic_with_cache(user_question: str) -> anthropic.types.Message:
    """
    Utilise le prompt caching d'Anthropic pour le system prompt.
    Apr√®s le premier appel, le system prompt est mis en cache (√©conomie ~90%).
    """
    return client_anth.messages.create(
        model="claude-haiku-4-5-20251001",
        max_tokens=500,
        system=[
            {
                "type":       "text",
                "text":       SYSTEM_PROMPT_LONG,
                "cache_control": {"type": "ephemeral"},   # ‚Üê cache
            }
        ],
        messages=[{"role": "user", "content": user_question}],
    )


# ---------------------------------------------------------------------------
# 3. Cascade avec score de confiance
# ---------------------------------------------------------------------------

async def cascading_pipeline(query: str) -> tuple[str, str]:
    """
    Essaie d'abord le mod√®le rapide/pas cher.
    Si le LLM-juge estime que la r√©ponse est insuffisante, escalade vers le mod√®le puissant.
    Retourne (r√©ponse, mod√®le_utilis√©).
    """
    FAST_MODEL   = "gpt-4o-mini"
    STRONG_MODEL = "gpt-4o"
    CONFIDENCE_THRESHOLD = 0.75

    # Passe 1 : mod√®le rapide
    resp_fast = await aclient_oai.chat.completions.create(
        model=FAST_MODEL,
        messages=[{"role": "user", "content": query}],
        temperature=0.2,
        max_tokens=400,
    )
    answer_fast = resp_fast.choices[0].message.content

    # Juge : √©value la confiance
    judge_resp = await aclient_oai.chat.completions.create(
        model=FAST_MODEL,
        messages=[{
            "role": "user",
            "content": (
                f"Question : {query}\nR√©ponse : {answer_fast}\n\n"
                "Note la confiance de cette r√©ponse de 0.0 √† 1.0. "
                "R√©ponds uniquement avec un float."
            ),
        }],
        temperature=0,
    )
    try:
        confidence = float(judge_resp.choices[0].message.content.strip())
    except ValueError:
        confidence = 0.5

    if confidence >= CONFIDENCE_THRESHOLD:
        return answer_fast, FAST_MODEL

    # Passe 2 : escalade vers le mod√®le fort
    resp_strong = await aclient_oai.chat.completions.create(
        model=STRONG_MODEL,
        messages=[{"role": "user", "content": query}],
        temperature=0.2,
        max_tokens=800,
    )
    return resp_strong.choices[0].message.content, STRONG_MODEL


# ---------------------------------------------------------------------------
# 4. Circuit breaker multi-provider
# ---------------------------------------------------------------------------

class Provider(Enum):
    ANTHROPIC = "anthropic"
    OPENAI    = "openai"


@dataclass
class CircuitBreaker:
    provider:        Provider
    failure_window:  int   = 60       # secondes
    failure_limit:   int   = 5        # erreurs avant ouverture
    cooldown:        int   = 30       # secondes avant retry

    _failures:       deque = None
    _open_since:     float = 0.0

    def __post_init__(self):
        self._failures = deque()

    @property
    def is_open(self) -> bool:
        now = time.time()
        # Nettoie les erreurs hors fen√™tre
        while self._failures and now - self._failures[0] > self.failure_window:
            self._failures.popleft()
        # V√©rifie si le cooldown est √©coul√©
        if self._open_since and now - self._open_since < self.cooldown:
            return True
        self._open_since = 0.0
        return len(self._failures) >= self.failure_limit

    def record_failure(self):
        self._failures.append(time.time())
        if len(self._failures) >= self.failure_limit:
            self._open_since = time.time()

    def record_success(self):
        self._failures.clear()
        self._open_since = 0.0


# ---------------------------------------------------------------------------
# Demo
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    # 1. Comparaison de co√ªts
    compare_models(input_tokens=2_000, output_tokens=500)
    compare_models(input_tokens=10_000, output_tokens=1_000)

    # 2. Cascade (n√©cessite OPENAI_API_KEY)
    if os.getenv("OPENAI_API_KEY"):
        queries = [
            "Quelle est la capitale de la France ?",     # simple ‚Üí mod√®le rapide
            "Compare RLHF et DPO en d√©taillant les gradients et les implications pratiques.",  # complexe ‚Üí escalade
        ]
        for q in queries:
            answer, model = asyncio.run(cascading_pipeline(q))
            print(f"\n‚ùì {q[:60]}")
            print(f"ü§ñ [{model}] {answer[:120]}...")

    # 3. Circuit breaker demo
    cb = CircuitBreaker(Provider.ANTHROPIC, failure_limit=3, cooldown=5)
    for i in range(4):
        cb.record_failure()
        print(f"  Apr√®s {i+1} erreur(s) ‚Äî circuit ouvert : {cb.is_open}")
