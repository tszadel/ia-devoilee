package fr.szadel.ia.ch09;

import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.memory.chat.MessageWindowChatMemory;
import dev.langchain4j.model.anthropic.AnthropicChatModel;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.model.embedding.onnx.allminilml6v2.AllMiniLmL6V2EmbeddingModel;
import dev.langchain4j.model.openai.OpenAiChatModel;
import dev.langchain4j.rag.content.retriever.EmbeddingStoreContentRetriever;
import dev.langchain4j.service.AiServices;
import dev.langchain4j.service.SystemMessage;
import dev.langchain4j.store.embedding.EmbeddingStore;
import dev.langchain4j.store.embedding.EmbeddingStoreIngestor;
import dev.langchain4j.store.embedding.inmemory.InMemoryEmbeddingStore;
import dev.langchain4j.store.embedding.weaviate.WeaviateEmbeddingStore;

import java.util.List;

/**
 * Chapitre 9 ‚Äî Pipeline RAG complet avec LangChain4j.
 *
 * <p>Deux variantes propos√©es :
 * <ol>
 *   <li>{@link InMemoryRagPipeline} ‚Äî zero infrastructure, id√©al pour les tests</li>
 *   <li>{@link WeaviateRagPipeline} ‚Äî production avec Weaviate (docker-compose inclus)</li>
 * </ol>
 *
 * <p>R√©f√©rence livre : section 9.4 "RAG ‚Äî la bo√Æte √† outils du d√©veloppeur"
 */
public class RagPipeline {

    // -------------------------------------------------------------------------
    // Interface commune : le LLM ne voit pas le vector store utilis√©
    // -------------------------------------------------------------------------

    interface Assistant {
        @SystemMessage("""
            Tu es un assistant p√©dagogique expert en LLM.
            R√©ponds UNIQUEMENT en te basant sur le contexte fourni.
            Si la r√©ponse ne s'y trouve pas, dis-le explicitement.
            Cite tes sources entre [crochets].
            """)
        String answer(String question);
    }

    // -------------------------------------------------------------------------
    // Variante 1 : In-Memory ‚Äî aucune infrastructure requise
    // -------------------------------------------------------------------------

    public static class InMemoryRagPipeline {

        private final Assistant assistant;

        public InMemoryRagPipeline(String openAiApiKey, List<String> documents) {
            EmbeddingModel embeddingModel = new AllMiniLmL6V2EmbeddingModel();

            // Store en m√©moire : parfait pour les tests et les d√©mos
            EmbeddingStore<TextSegment> store = new InMemoryEmbeddingStore<>();

            // Indexation : d√©coupage ‚Üí embedding ‚Üí stockage en une passe
            var ingestor = EmbeddingStoreIngestor.builder()
                .documentSplitter(DocumentSplitters.recursive(400, 50))
                .embeddingModel(embeddingModel)
                .embeddingStore(store)
                .build();

            documents.stream()
                .map(Document::from)
                .forEach(ingestor::ingest);

            // Retriever : top-5 par similarit√© cosinus
            var retriever = EmbeddingStoreContentRetriever.builder()
                .embeddingStore(store)
                .embeddingModel(embeddingModel)
                .maxResults(5)
                .minScore(0.6)   // filtre les chunks peu pertinents
                .build();

            this.assistant = AiServices.builder(Assistant.class)
                .chatLanguageModel(
                    OpenAiChatModel.builder()
                        .apiKey(openAiApiKey)
                        .modelName("gpt-4o-mini")
                        .temperature(0.2)
                        .build()
                )
                .contentRetriever(retriever)
                .chatMemory(MessageWindowChatMemory.withMaxMessages(10))
                .build();
        }

        public String ask(String question) {
            return assistant.answer(question);
        }
    }

    // -------------------------------------------------------------------------
    // Variante 2 : Weaviate ‚Äî production
    // -------------------------------------------------------------------------

    public static class WeaviateRagPipeline {

        private final Assistant assistant;

        /**
         * @param anthropicApiKey  cl√© API Anthropic
         * @param weaviateHost     ex. "localhost:8080" ou "cluster.weaviate.io"
         * @param weaviateScheme   "http" (local) ou "https" (cloud)
         */
        public WeaviateRagPipeline(String anthropicApiKey,
                                   String weaviateHost,
                                   String weaviateScheme) {
            EmbeddingModel embeddingModel = new AllMiniLmL6V2EmbeddingModel();

            // Weaviate : self-hosted ou cloud
            EmbeddingStore<TextSegment> store = WeaviateEmbeddingStore.builder()
                .scheme(weaviateScheme)
                .host(weaviateHost)
                .className("DocChunk")
                .build();

            var retriever = EmbeddingStoreContentRetriever.builder()
                .embeddingStore(store)
                .embeddingModel(embeddingModel)
                .maxResults(5)
                .minScore(0.65)
                .build();

            this.assistant = AiServices.builder(Assistant.class)
                .chatLanguageModel(
                    AnthropicChatModel.builder()
                        .apiKey(anthropicApiKey)
                        .modelName("claude-haiku-4-5-20251001")
                        .maxTokens(800)
                        .build()
                )
                .contentRetriever(retriever)
                .chatMemory(MessageWindowChatMemory.withMaxMessages(10))
                .build();
        }

        /**
         * Indexe des documents dans Weaviate.
         * √Ä appeler une fois lors de l'initialisation du pipeline.
         */
        public void ingest(List<String> documents) {
            EmbeddingModel embeddingModel = new AllMiniLmL6V2EmbeddingModel();
            EmbeddingStore<TextSegment> store = WeaviateEmbeddingStore.builder()
                .scheme("http").host("localhost:8080")
                .className("DocChunk")
                .build();

            var ingestor = EmbeddingStoreIngestor.builder()
                .documentSplitter(DocumentSplitters.recursive(400, 50))
                .embeddingModel(embeddingModel)
                .embeddingStore(store)
                .build();

            documents.stream()
                .map(Document::from)
                .forEach(ingestor::ingest);
        }

        public String ask(String question) {
            return assistant.answer(question);
        }
    }

    // -------------------------------------------------------------------------
    // Demo autonome
    // -------------------------------------------------------------------------

    public static void main(String[] args) {
        var documents = List.of(
            "La tokenisation BPE divise les mots rares en sous-unit√©s fr√©quentes.",
            "L'attention multi-t√™tes permet au mod√®le de se concentrer sur plusieurs parties simultan√©ment.",
            "Le RAG combine recherche documentaire et g√©n√©ration de texte pour ancrer les r√©ponses.",
            "LoRA r√©duit le nombre de param√®tres entra√Ænables en factorisant les mises √† jour de matrices.",
            "La fen√™tre de contexte limite la quantit√© de texte qu'un LLM traite en une seule fois.",
            "Le temperature contr√¥le l'al√©atoire de la g√©n√©ration : 0 = d√©terministe, 2 = tr√®s cr√©atif."
        );

        String apiKey = System.getenv("OPENAI_API_KEY");
        if (apiKey == null) {
            System.err.println("‚ö†Ô∏è  OPENAI_API_KEY manquante ‚Äî d√©mo d√©sactiv√©e.");
            return;
        }

        var pipeline = new InMemoryRagPipeline(apiKey, documents);

        var questions = List.of(
            "Comment fonctionne le RAG ?",
            "Qu'est-ce que LoRA ?",
            "Quel est le r√¥le du temperature ?"
        );

        for (var q : questions) {
            System.out.printf("%n‚ùì %s%nüí¨ %s%n", q, pipeline.ask(q));
        }
    }
}
